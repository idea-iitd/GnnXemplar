{
  "in_context_examples": "\n            In Context Examples: \n            # ===Example of Good Explanation ===\n            Concept node: 42\n            Concept node description: {\n            'node_id': 42,\n            'features': [0.1, 0.2, 0.3, 0.0, 0.5, 0.0, 0.9],\n            '1-hop': {'neighbor_class_freq': {1: 0.75, 2: 0.25}},\n            '2-hop': {'neighbor_class_freq': {1: 0.60, 2: 0.20, 3: 0.20}}\n            }\n            Cluster nodes: [101, 102, 103, 104]\n            Non-cluster nodes: [201, 202, 203, 204]\n            Descriptions of cluster nodes:\n            Node 101: {'node_id': 101, 'features': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], '1-hop': {'neighbor_class_freq': {1: 1.0}}, '2-hop': {'neighbor_class_freq': {1: 0.80, 3: 0.20}}}\n            Node 102: {'node_id': 102, 'features': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], '1-hop': {'neighbor_class_freq': {1: 0.75, 2: 0.25}}, '2-hop': {'neighbor_class_freq': {1: 0.60, 2: 0.40}}}\n            Node 103: {'node_id': 103, 'features': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], '1-hop': {'neighbor_class_freq': {1: 0.80, 3: 0.20}}, '2-hop': {'neighbor_class_freq': {1: 0.70, 3: 0.30}}}\n            Node 104: {'node_id': 104, 'features': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], '1-hop': {'neighbor_class_freq': {1: 0.90, 2: 0.10}}, '2-hop': {'neighbor_class_freq': {1: 0.85, 2: 0.15}}}\n            Descriptions of non-cluster nodes:\n            Node 201: {'node_id': 201, 'features': [0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0], '1-hop': {'neighbor_class_freq': {2: 1.0}}, '2-hop': {'neighbor_class_freq': {2: 0.90, 3: 0.10}}}\n            Node 202: {'node_id': 202, 'features': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], '1-hop': {'neighbor_class_freq': {3: 1.0}}, '2-hop': {'neighbor_class_freq': {3: 1.0}}}\n            Node 203: {'node_id': 203, 'features': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], '1-hop': {'neighbor_class_freq': {2: 0.60, 3: 0.40}}, '2-hop': {'neighbor_class_freq': {2: 0.50, 3: 0.50}}}\n            Node 204: {'node_id': 204, 'features': [0.2, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0], '1-hop': {'neighbor_class_freq': {3: 0.70, 1: 0.30}}, '2-hop': {'neighbor_class_freq': {3: 0.60, 1: 0.40}}}\n\n            # === Output 1 ===\n            {\n            \"rules\": [\n                \"node['1-hop']['neighbor_class_freq'].get(1, 0) >= 0.75\",\n                \"node['2-hop']['neighbor_class_freq'].get(1, 0) >= 0.6\"\n            ],\n            \"interpretation\": \"Cluster nodes have at least 75% of immediate neighbors of class 1 and at least 60% of 2-hop neighbors of class 1.\"\n            }\n            def classify_node(node_description):\n                if node_description['1-hop']['neighbor_class_freq'].get(1, 0) >= 0.75 and                 node_description['2-hop']['neighbor_class_freq'].get(1, 0) >= 0.6:\n                    return 1\n                else:\n                    return 0\n            \n            Verdict: 95% accuracy\n            IMPORTANT:Observe how in the output rules we have only focused on using the neghborhood class distribution and ignored the node features.\n\n            # === Example of Bad Explanation===\n            For the same input as above, the LLM generated the following code:\n            LLM Rule:\n            ```python\n            if node['features'][0] >= 0.875:\n                return 1\n            else:\n                return 0\n            Verdict: 0% Accuracy. {\"Error\": \"The rule is based on the feature vector, not the neighborhood class distribution. You are not allowed to use features in the rules.\"}\n\n            Refer to the in context examples to generate the rules.\n            ",
  "class_description": {
    "0": "cs.AI - Artificial Intelligence",
    "1": "cs.AR - Hardware Architecture",
    "2": "cs.CC - Computational Complexity",
    "3": "cs.CE - Computational Engineering, Finance, and Science",
    "4": "cs.CG - Computational Geometry",
    "5": "cs.CL - Computation and Language",
    "6": "cs.CR - Cryptography and Security",
    "7": "cs.CV - Computer Vision and Pattern Recognition",
    "8": "cs.CY - Computers and Society",
    "9": "cs.DB - Databases",
    "10": "cs.DC - Distributed, Parallel, and Cluster Computing",
    "11": "cs.DL - Digital Libraries",
    "12": "cs.DM - Discrete Mathematics",
    "13": "cs.DS - Data Structures and Algorithms",
    "14": "cs.ET - Emerging Technologies",
    "15": "cs.FL - Formal Languages and Automata Theory",
    "16": "cs.GL - General Literature",
    "17": "cs.GR - Graphics",
    "18": "cs.GT - Computer Science and Game Theory",
    "19": "cs.HC - Human-Computer Interaction",
    "20": "cs.IR - Information Retrieval",
    "21": "cs.IT - Information Theory",
    "22": "cs.LG - Machine Learning",
    "23": "cs.LO - Logic in Computer Science",
    "24": "cs.MA - Multiagent Systems",
    "25": "cs.MM - Multimedia",
    "26": "cs.MS - Mathematical Software",
    "27": "cs.NA - Numerical Analysis",
    "28": "cs.NE - Neural and Evolutionary Computing",
    "29": "cs.NI - Networking and Internet Architecture",
    "30": "cs.OH - Other Computer Science",
    "31": "cs.OS - Operating Systems",
    "32": "cs.PF - Performance",
    "33": "cs.PL - Programming Languages",
    "34": "cs.RO - Robotics",
    "35": "cs.SC - Symbolic Computation",
    "36": "cs.SD - Sound",
    "37": "cs.SE - Software Engineering",
    "38": "cs.SI - Social and Information Networks",
    "39": "cs.SY - Systems and Control"
  },
  "dataset_description": "Graph: The ogbn-arxiv dataset is a directed graph, representing the citation network between all Computer Science (CS) arXiv papers indexed by MAG. Each node is an arXiv paper and each directed edge indicates that one paper cites another one. Each paper comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract. The embeddings of individual words are computed by running the skip-gram model over the MAG corpus. Prediction task: The task is to predict the 40 subject areas of arXiv CS papers, e.g., cs.AI, cs.LG, and cs.OS, which are manually determined (i.e., labeled) by the paperâ€™s authors and arXiv moderators. Formally, the task is to predict the primary categories of the arXiv papers, which is formulated as a 40-class classification problem."
}