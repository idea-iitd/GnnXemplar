import os

import pandas as pd
import numpy as np
import torch
import torch_geometric as pyg
import torch_geometric.transforms as T
from torch_geometric import datasets
from torch_geometric.data import Data
from ogb.nodeproppred import PygNodePropPredDataset
from torch_geometric.utils import dense_to_sparse

import gnns
from utils import seed_everything

seed = 0
seed_everything(seed)


def load_dataset(dataset_name: str, trained_model=True):
    """
    This function loads the dataset and the GNN trained on it.
    It then computes the GNN's predictions and node embeddings.

    Args:
        dataset_name (str): Dataset name

    Returns:
        data (pyg.data.Data): Graph object
        gnn_pred (np.ndarray): Black box GNN's predictions
        node_embeddings (np.ndarray): Node embeddings generated by the black box GNN
    """

    if dataset_name == "bashapes":
        # gnnexplainer's dataset
        # dataset = torch.load("data/bashapes/data.pt")
        dataset = torch.load("data/bashapes/data.pt")

        x = torch.FloatTensor(dataset["feat"])  # (1, 700, 10)
        adj = torch.FloatTensor(dataset["adj"])  # (1, 700, 700)
        labels = torch.LongTensor(dataset["labels"])  # (1, 700)
        # labels map
        # 1: middle
        # 2: base
        # 3: tip

        # Reshaping and sparsifying
        x = x.squeeze(0)
        edge_index = dense_to_sparse(adj=adj)[0]
        y = labels.squeeze(0)

        # pyg's data object
        data = Data(x=x, edge_index=edge_index, y=y)
        del dataset, x, adj, labels, edge_index, y

        # Train and test masks
        indices = torch.load("data/bashapes/indices.pt")
        indices["val"] = indices["test"][:len(indices["test"]) // 2]
        indices["test"] = indices["test"][len(indices["test"]) // 2:]

        data.train_mask = torch.full((data.num_nodes,), False)
        data.val_mask = torch.full((data.num_nodes,), False)
        data.test_mask = torch.full((data.num_nodes,), False)

        data.train_mask[indices["train"]] = True
        data.val_mask[indices["val"]] = True
        data.test_mask[indices["test"]] = True

        # Model
        model = gnns.Gnnexplainer_GCN(
            input_dim=10,
            hidden_dim=20,
            embedding_dim=20,
            label_dim=4,
            num_layers=3,
            bn=False,
        )
        model.load_state_dict(torch.load("data/bashapes/model.pt"))
        model.eval()

        with torch.no_grad():
            # Shapes: out=(1, 700, 4), node_embeddings=(1, 700, 60)
            out, node_embeddings = model(
                x=data.x, edge_index=data.edge_index)
            gnn_pred = out.argmax(dim=-1).squeeze(0)
            node_embeddings = node_embeddings.squeeze(0)

    elif dataset_name == "TAGCora":  # Text attributed version of cora
        # * >>> Load the data
        cur_path = os.path.dirname(__file__)
        # cur_path = os.path.join(cur_path, 'data/TAGCora')
        cur_path = 'data/TAGCora'
        path = os.path.join(cur_path, "cora.pt")
        dataset = torch.load(path)
        dataset.num_classes = len(dataset.label_names)

        nx_g = pyg.utils.to_networkx(dataset, to_undirected=True)
        edge_index = torch.tensor(list(nx_g.edges())).T
        # make this graph disconnected , that is, 0 edges
        # edge_index = torch.tensor([[0, 0], [0, 0]], dtype=torch.long)
        category_desc = pd.read_csv(
            os.path.join(cur_path, "categories.csv"), sep=","
        ).values

        # pyg's data object
        data = Data(x=dataset.x, edge_index=edge_index, y=dataset.y)
        del edge_index
        data.train_mask = dataset.train_masks[0]
        data.val_mask = dataset.val_masks[0]
        data.test_mask = dataset.test_masks[0]
        data.raw_text = dataset.raw_text
        data.label_names = dataset.label_names
        data.category_names = dataset.category_names

        # FIXIT: feature dimension mismatch
        # pdb.set_trace()

        # dataset = datasets.Planetoid(
        #     root="data", name="CORA", split="public", force_reload=True,)
        # data = dataset[0]
        if not trained_model:
            return dataset, data

        # * >>> Model
        model = gnns.CORA_GAT(in_channels=dataset.num_features,
                              num_classes=dataset.num_classes)
        model.load_state_dict(torch.load("data/TAGCora/state_dict.pt"))
        model.eval()

        with torch.no_grad():
            out, node_embeddings = model(data.x, data.edge_index)
            gnn_pred = out.argmax(dim=-1)

    elif dataset_name == "WikiCS":
        """
        Example for loading the WikiCS dataset, splitting it, 
        and using a pre-trained GNN model from gnns.py
        """
        # 1) Load the dataset
        dataset = datasets.WikiCS(root="data/WikiCS")
        # WikiCS returns a single data object with multiple masks.
        data = dataset[0]

        data.train_mask = data.train_mask[:, 0]
        data.val_mask = data.val_mask[:, 0]
        # test_mask is consistent across all splits:
        data.test_mask = data.test_mask  # (already a single column)

        model = gnns.WikiCS_GCN(
            in_channels=dataset.num_features,
            num_classes=dataset.num_classes
        )
        # Alternatively, for a GAT:
        # model = gnns.WikiCS_GAT(in_channels=dataset.num_features,
        #                         num_classes=dataset.num_classes)

        # 4) Load your trained model parameters
        model.load_state_dict(torch.load("data/WikiCS/state_dict.pt"))
        model.eval()

        # 5) Generate GNN predictions + embeddings
        with torch.no_grad():
            out, node_embeddings = model(data.x, data.edge_index)
            gnn_pred = out.argmax(dim=-1)

        # return data, model, gnn_pred.numpy(), node_embeddings.numpy()

    elif dataset_name == "arxiv":
        dataset = PygNodePropPredDataset(
            name='ogbn-arxiv', root='data/ogbn-arxiv', transform=T.ToSparseTensor())
        data = dataset[0]

        if hasattr(data, 'adj_t'):
            # Check if data.adj_t is a CSR tensor:
            if hasattr(data.adj_t, "layout") and data.adj_t.layout == torch.sparse_csr:
                # Convert CSR to COO tensor:
                coo = data.adj_t.to_sparse_coo().coalesce()
                data.edge_index = coo.indices()
            elif callable(getattr(data.adj_t, "to_symmetric", None)):
                data.adj_t = data.adj_t.to_symmetric()
                data.edge_index = data.adj_t.to_torch_sparse_coo_tensor().coalesce().indices()
            else:
                # Otherwise assume it's a dense tensor:
                data.edge_index = (data.adj_t != 0).nonzero(
                    as_tuple=False).t().contiguous()
        elif hasattr(data, 'edge_index'):
            pass
        else:
            raise AttributeError("Data has neither 'adj_t' nor 'edge_index'.")

        # make the graph undirected
        data.edge_index = pyg.utils.to_undirected(
            data.edge_index, data.num_nodes)
        num_nodes = data.num_nodes
        split_idx = dataset.get_idx_split()
        data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)
        data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)
        data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)
        data.train_mask[split_idx['train']] = True
        data.val_mask[split_idx['valid']] = True
        data.test_mask[split_idx['test']] = True

        data.y = data.y.squeeze(1)

        from gnns import Arxiv_GCN
        model = Arxiv_GCN(
            in_channels=dataset.num_features,
            # num_classes=dataset.num_classes,
            out_channels=dataset.num_classes,
            # hidden_dim=128
            hidden_channels=256
        )

        # Load your trained model parameters
        model.load_state_dict(torch.load("data/arxiv/model.pt"))
        model.eval()

        with torch.no_grad():
            out, node_embeddings = model(data.x, data.edge_index)
            gnn_pred = out.argmax(dim=-1)
        gnn_pred = gnn_pred.cpu().numpy()
        node_embeddings = node_embeddings.cpu().numpy()
        return data, model, gnn_pred, node_embeddings

    elif dataset_name == "Citeseer":
        # Load Citeseer dataset using Planetoid:
        dataset = datasets.Planetoid(root="data/Citeseer", name="Citeseer")
        data = dataset[0]

        # Citeseer (from Planetoid) already has train_mask, val_mask, test_mask
        # Create a model and compute GNN predictions:
        from gnns import Citeseer_GCN  # Assuming you have a GCN defined in gnns.py
        model = Citeseer_GCN(in_channels=dataset.num_features,
                             hidden_channels=64,
                             out_channels=dataset.num_classes,
                             num_layers=3,
                             dropout=0.5)
        # Optionally load a pretrained model here:
        model.load_state_dict(torch.load("data/Citeseer/model.pt"))
        model.eval()

        with torch.no_grad():
            out, node_embeddings = model(data.x, data.edge_index)
            gnn_pred = out.argmax(dim=-1)

        return data, model, gnn_pred.cpu().numpy(), node_embeddings.cpu().numpy()

    elif dataset_name == "questions":
        # Load the Roman Empire dataset using HeterophilousGraphDataset.
        # Adjust "RomanEmpire" in name and root as needed.

        dataset = datasets.HeterophilousGraphDataset(
            root="data/questions", name="Questions")
        data = dataset[0]
        print("Number of classes: ", dataset.num_classes)

        # Check if the dataset already has train/val/test masks.
        # If not, you can create random splits. For example:
        num_nodes = data.num_nodes
        if not hasattr(data, 'train_mask'):
            # Create random splits (e.g., 60/20/20)
            perm = torch.randperm(num_nodes)
            train_size = int(0.6 * num_nodes)
            val_size = int(0.2 * num_nodes)
            data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)
            data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)
            data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)
            data.train_mask[perm[:train_size]] = True
            data.val_mask[perm[train_size:train_size+val_size]] = True
            data.test_mask[perm[train_size+val_size:]] = True

        # Instantiate the model (using your generic GCN, for example).
        from gnns import GCN
        model = GCN(in_channels=dataset.num_features,
                    hidden_channels=64,
                    out_channels=dataset.num_classes,
                    num_layers=3,
                    dropout=0.5)

        if os.path.exists("data/questions/model.pt"):
            model.load_state_dict(torch.load("data/questions/model.pt"))
        model.eval()
        with torch.no_grad():
            out, node_embeddings = model(data.x, data.edge_index)
            gnn_pred = out.argmax(dim=-1)
        return data, model, gnn_pred.cpu().numpy(), node_embeddings.cpu().numpy()

    elif dataset_name == "amazonratings":
        # Load the Amazonâ€‘Ratings dataset via HeterophilousGraphDataset
        dataset = datasets.HeterophilousGraphDataset(
            root="data/amazonratings", name="Amazon-ratings")
        data = dataset[0]

        # If the dataset does not already have train/val/test masks, create random splits:
        num_nodes = data.num_nodes
        if not hasattr(data, 'train_mask'):
            perm = torch.randperm(num_nodes)
            train_size = int(0.6 * num_nodes)
            val_size = int(0.2 * num_nodes)
            data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)
            data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)
            data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)
            data.train_mask[perm[:train_size]] = True
            data.val_mask[perm[train_size:train_size+val_size]] = True
            data.test_mask[perm[train_size+val_size:]] = True

        # Ensure labels are 1D:
        data.y = data.y.squeeze() if data.y.dim() > 1 else data.y

        from gnns import GCN
        model = GCN(in_channels=dataset.num_features,
                    hidden_channels=64,
                    out_channels=dataset.num_classes,
                    num_layers=3,
                    dropout=0.5)
        if os.path.exists("data/amazonratings/model.pt"):
            model.load_state_dict(torch.load("data/amazonratings/model.pt"))
        model.eval()
        with torch.no_grad():
            out, node_embeddings = model(data.x, data.edge_index)
            gnn_pred = out.argmax(dim=-1)
        return data, model, gnn_pred.cpu().numpy(), node_embeddings.cpu().numpy()

    elif dataset_name == "minesweeper":
        dataset = datasets.HeterophilousGraphDataset(
            root="data/minesweeper", name="Minesweeper")
        data = dataset[0]

        # If train/val/test masks are not provided, create random splits:
        num_nodes = data.num_nodes
        if not hasattr(data, 'train_mask'):
            perm = torch.randperm(num_nodes)
            train_size = int(0.6 * num_nodes)
            val_size = int(0.2 * num_nodes)
            data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)
            data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)
            data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)
            data.train_mask[perm[:train_size]] = True
            data.val_mask[perm[train_size:train_size+val_size]] = True
            data.test_mask[perm[train_size+val_size:]] = True

        # Ensure labels are 1D:
        data.y = data.y.squeeze() if data.y.dim() > 1 else data.y

        from gnns import GCN
        model = GCN(in_channels=dataset.num_features,
                    hidden_channels=64,
                    out_channels=dataset.num_classes,
                    num_layers=3,
                    dropout=0.5)
        if os.path.exists("data/minesweeper/model.pt"):
            model.load_state_dict(torch.load("data/minesweeper/model.pt"))
        model.eval()
        with torch.no_grad():
            out, node_embeddings = model(data.x, data.edge_index)
            gnn_pred = out.argmax(dim=-1)
        return data, model, gnn_pred.cpu().numpy(), node_embeddings.cpu().numpy()

    else:
        raise ValueError(f"Invalid dataset name: {dataset_name}")

    return data, model, gnn_pred.numpy(), node_embeddings.numpy()
